{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "from cub_data import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "#     if torch.cuda.is_available():\n",
    "#         x = x.cuda()\n",
    "    global device\n",
    "    return x.to(device)\n",
    "\n",
    "def idx2onehot(idx, n):\n",
    "\n",
    "    assert idx.size(1) == 1\n",
    "    assert torch.max(idx).data[0] < n\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx.data, 1)\n",
    "    onehot = to_var(onehot)\n",
    "    \n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer_sizes, latent_size, decoder_layer_sizes, regressor_layer_sizes, attributes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "        self.num_labels = attributes.shape[0]\n",
    "        self.attribute_size = attributes.shape[1]\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(attributes))\n",
    "\n",
    "        \n",
    "        self.encoder = Encoder(encoder_layer_sizes, latent_size, self.num_labels)\n",
    "        self.decoder = Decoder(decoder_layer_sizes, latent_size, self.attribute_size, self.embeddings)\n",
    "        self.regressor = Regressor(regressor_layer_sizes, attributes)\n",
    "        \n",
    "        \"\"\"\n",
    "        Grouping the model's parameters: separating encoder, decoder, and discriminator\n",
    "        \"\"\"\n",
    "        self.encoder_params = chain(\n",
    "            self.encoder.parameters()\n",
    "        )\n",
    "\n",
    "        self.decoder_params = chain(\n",
    "            self.decoder.parameters()\n",
    "        )\n",
    "\n",
    "        self.vae_params = chain(\n",
    "            self.encoder_params, self.decoder_params\n",
    "        )\n",
    "        self.vae_params = filter(lambda p: p.requires_grad, self.vae_params)\n",
    "\n",
    "        self.regressor_params = filter(lambda p: p.requires_grad, self.regressor.parameters())\n",
    "        \n",
    "    def sample_z(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std*eps; eps ~ N(0, I)\n",
    "        \"\"\"\n",
    "        batch_size = mu.size(0)\n",
    "        eps = to_var(torch.randn([batch_size, self.latent_size]))\n",
    "        return mu + torch.exp(logvar/2) * eps\n",
    "\n",
    "    def sample_z_prior(self, bsize):\n",
    "        \"\"\"\n",
    "        Sample z ~ p(z) = N(0, I)\n",
    "        \"\"\"\n",
    "        z = to_var(torch.randn(bsize, self.latent_size))\n",
    "        return z\n",
    "\n",
    "    def sample_c_prior(self, bsize):\n",
    "        \"\"\"\n",
    "        Sample c ~ p(c) = Cat([0.5, 0.5])\n",
    "        \"\"\"\n",
    "        c = to_var(\n",
    "            torch.LongTensor(np.random.randint(0, self.num_labels, (bsize,1)))\n",
    "        )\n",
    "        return c\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        -------\n",
    "        c: whether to sample `c` from prior or use what is provided.\n",
    "        Returns:\n",
    "        --------\n",
    "        recon_loss: reconstruction loss of VAE.\n",
    "        kl_loss: KL-div loss of VAE.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        means, log_var = self.encoder(x)\n",
    "\n",
    "        z = self.sample_z(means, log_var)\n",
    "\n",
    "        if c is None:\n",
    "            c = self.sample_c_prior(batch_size)\n",
    "        \n",
    "        recon_x = self.decoder(z, c)\n",
    "        \n",
    "        recon_loss = F.mse_loss(recon_x, x, size_average=True)\n",
    "        kl_loss = torch.mean(0.5 * torch.sum(torch.exp(log_var) + means**2 - 1 - log_var, 1))\n",
    "\n",
    "        return recon_loss, kl_loss\n",
    "\n",
    "    def synthesize_examples(self, n=1):\n",
    "\n",
    "        batch_size = n\n",
    "        z = self.sample_z_prior(batch_size)\n",
    "        c = self.sample_c_prior(batch_size)\n",
    "        \n",
    "        recon_x = self.decoder(z, c)\n",
    "\n",
    "        return recon_x\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_size, num_labels):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.MLP = nn.Sequential()\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate( zip(layer_sizes[:-1], layer_sizes[1:]) ):\n",
    "            self.MLP.add_module(name=\"L%i\"%(i), module=nn.Linear(in_size, out_size))\n",
    "            self.MLP.add_module(name=\"A%i\"%(i), module=nn.ReLU())\n",
    "#             self.MLP.add_module(name=\"BN%i\"%(i), module=nn.BatchNorm1d(out_size))\n",
    "\n",
    "\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.MLP(x)\n",
    "\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = torch.log(F.softplus(self.linear_var(x)))\n",
    "#         log_vars = self.linear_var(x)\n",
    "\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_size, attribute_size, embeddings):\n",
    "\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embeddings = embeddings\n",
    "        self.MLP1 = nn.Sequential()\n",
    "        self.MLP2 = nn.Sequential()\n",
    "\n",
    "        input_size = latent_size + attribute_size\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate( zip([input_size]+layer_sizes[:-1], layer_sizes)):\n",
    "            self.MLP1.add_module(name=\"L%i\"%(i), module=nn.Linear(in_size, out_size))\n",
    "            if i+1 < len(layer_sizes):\n",
    "                self.MLP1.add_module(name=\"A%i\"%(i), module=nn.ReLU())\n",
    "#                 self.MLP.add_module(name=\"BN%i\"%(i), module=nn.BatchNorm1d(out_size))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        for i, (in_size, out_size) in enumerate( zip([input_size]+layer_sizes[:-1], layer_sizes)):\n",
    "            self.MLP2.add_module(name=\"L%i\"%(i), module=nn.Linear(in_size, out_size))\n",
    "            if i+1 < len(layer_sizes):\n",
    "                self.MLP2.add_module(name=\"A%i\"%(i), module=nn.ReLU())\n",
    "#                 self.MLP.add_module(name=\"BN%i\"%(i), module=nn.BatchNorm1d(out_size))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    def forward(self, z, c):\n",
    "        \n",
    "        a = self.embeddings(c.view(-1))\n",
    "        z = torch.cat((z, a), dim=-1)\n",
    "\n",
    "        x1 = self.MLP1(z)\n",
    "        x2 = self.MLP2(z)\n",
    "\n",
    "        return x1 + x2\n",
    "    \n",
    "class Regressor(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, attributes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_labels = attributes.shape[0]\n",
    "        self.attribute_size = attributes.shape[1]\n",
    "        self.attributes = torch.FloatTensor(attributes)\n",
    "        self.attributes = self.attributes.view((1, self.num_labels, self.attribute_size))\n",
    "        \n",
    "        self.MLP = nn.Sequential()\n",
    "        i=0\n",
    "        for i, (in_size, out_size) in enumerate( zip(layer_sizes[:-1], layer_sizes[1:]) ):\n",
    "            self.MLP.add_module(name=\"L%i\"%(i), module=nn.Linear(in_size, out_size))\n",
    "            self.MLP.add_module(name=\"A%i\"%(i), module=nn.ReLU())\n",
    "#             self.MLP.add_module(name=\"BN%i\"%(i), module=nn.BatchNorm1d(out_size))\n",
    "\n",
    "        self.MLP.add_module(name=\"L%i\"%(i+1), module=nn.Linear(layer_sizes[-1], self.attribute_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        a = self.MLP(x)\n",
    "        # Reshape predicted attribute because broadcasting is not supported\n",
    "        # a has shape               batch_size x   1 x 312\n",
    "        # attributes have shape:             1 x 200 x 312\n",
    "        a = a.view((-1, 1, self.attribute_size))\n",
    "        # logits of shape:          batch_size x 200\n",
    "        logits = nn.CosineSimilarity(dim=2, eps=1e-6)(a, self.attributes)\n",
    "        # log of predictions shape: batch_size x 200\n",
    "        c_hat = F.log_softmax(logits, dim=-1)\n",
    "        return c_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CUB_data('xlsa17/data/CUB/att_splits.mat', 'xlsa17/data/CUB/res101.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, c_train = data.sets['trainval_X'], data.sets['trainval_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, c_val = data.sets['val_X'], data.sets['val_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seen_x, test_seen_c = data.sets['test_seen_X'], data.sets['test_seen_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unseen_x, test_unseen_c = data.sets['test_unseen_X'], data.sets['test_unseen_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE([2048, 512, 512], 78, [512, 2048], [2048, 512], data.class_attributes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('weights/vae_relu_wu3_40_40_40.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (embeddings): Embedding(200, 312)\n",
       "  (encoder): Encoder(\n",
       "    (MLP): Sequential(\n",
       "      (L0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (A0): ReLU()\n",
       "      (L1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (A1): ReLU()\n",
       "    )\n",
       "    (linear_means): Linear(in_features=512, out_features=78, bias=True)\n",
       "    (linear_var): Linear(in_features=512, out_features=78, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embeddings): Embedding(200, 312)\n",
       "    (MLP1): Sequential(\n",
       "      (L0): Linear(in_features=390, out_features=512, bias=True)\n",
       "      (A0): ReLU()\n",
       "      (L1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    )\n",
       "    (MLP2): Sequential(\n",
       "      (L0): Linear(in_features=390, out_features=512, bias=True)\n",
       "      (A0): ReLU()\n",
       "      (L1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (regressor): Regressor(\n",
       "    (MLP): Sequential(\n",
       "      (L0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (A0): ReLU()\n",
       "      (L1): Linear(in_features=512, out_features=312, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment():\n",
    "    X_syn_train = []\n",
    "    c_syn_train = []\n",
    "    for label in range(200):\n",
    "        batch_size = 100\n",
    "        label_array = np.zeros((batch_size, 1), dtype='int32') + label\n",
    "        \n",
    "        real_idx = np.where(c_train.reshape(-1)==label)[0]\n",
    "        x_real = x_train[real_idx]\n",
    "        batch_size -= x_real.shape[0]\n",
    "        \n",
    "        z = model.sample_z_prior(batch_size)\n",
    "        c = to_var(torch.LongTensor(label_array[:batch_size]))\n",
    "        x = model.decoder.forward(z, c)\n",
    "        if device.type == 'cpu':\n",
    "            X_syn_train.append(x.data.numpy())\n",
    "        else:\n",
    "            X_syn_train.append(x.data.cpu().numpy())\n",
    "        X_syn_train.append(x_real)\n",
    "        c_syn_train.append(label_array)\n",
    "    return np.concatenate(X_syn_train, axis=0), np.concatenate(c_syn_train, axis=0)\n",
    "\n",
    "def synthesize(bs):\n",
    "    X_syn_train = []\n",
    "    c_syn_train = []\n",
    "    for label in range(200):\n",
    "        batch_size = bs\n",
    "        label_array = np.zeros((batch_size, 1), dtype='int32') + label\n",
    "        \n",
    "        z = model.sample_z_prior(batch_size)\n",
    "        c = to_var(torch.LongTensor(label_array))\n",
    "        x = model.decoder.forward(z, c)\n",
    "        if device.type == 'cpu':\n",
    "            X_syn_train.append(x.data.numpy())\n",
    "        else:\n",
    "            X_syn_train.append(x.data.cpu().numpy())\n",
    "        c_syn_train.append(label_array)\n",
    "    return np.concatenate(X_syn_train, axis=0), np.concatenate(c_syn_train, axis=0)\n",
    "\n",
    "def synthesize_uns(bs):\n",
    "    new = {}\n",
    "    for x,y in zip(test_unseen_x,test_unseen_c):\n",
    "        try:\n",
    "            new[int(y[0])].append(x)\n",
    "        except:\n",
    "            new[int(y[0])] = [x]\n",
    "    un_cls = list(new.keys())\n",
    "\n",
    "    X_syn_train = []\n",
    "    c_syn_train = []\n",
    "    for label in un_cls:\n",
    "        batch_size = bs\n",
    "        label_array = np.zeros((batch_size, 1), dtype='int32') + label\n",
    "        \n",
    "        z = model.sample_z_prior(batch_size)\n",
    "        c = to_var(torch.LongTensor(label_array))\n",
    "        x = model.decoder.forward(z, c)\n",
    "        if device.type == 'cpu':\n",
    "            X_syn_train.append(x.data.numpy())\n",
    "        else:\n",
    "            X_syn_train.append(x.data.cpu().numpy())\n",
    "        c_syn_train.append(label_array)\n",
    "    return np.concatenate(X_syn_train, axis=0), np.concatenate(c_syn_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, c = synthesize(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = {i:1.0 for i in seen_cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    if i not in seen_cls:\n",
    "        cls_weights[i] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del nnc\n",
    "nnc = MLPClassifier(hidden_layer_sizes=512, learning_rate='adaptive', early_stopping=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.99299838\n",
      "Validation score: 0.220500\n",
      "Iteration 2, loss = 2.69652770\n",
      "Validation score: 0.327500\n",
      "Iteration 3, loss = 2.21362648\n",
      "Validation score: 0.412500\n",
      "Iteration 4, loss = 1.85767722\n",
      "Validation score: 0.470500\n",
      "Iteration 5, loss = 1.57754455\n",
      "Validation score: 0.511000\n",
      "Iteration 6, loss = 1.34994003\n",
      "Validation score: 0.546000\n",
      "Iteration 7, loss = 1.15472538\n",
      "Validation score: 0.586500\n",
      "Iteration 8, loss = 1.01374667\n",
      "Validation score: 0.624500\n",
      "Iteration 9, loss = 0.88019628\n",
      "Validation score: 0.639000\n",
      "Iteration 10, loss = 0.77421853\n",
      "Validation score: 0.674000\n",
      "Iteration 11, loss = 0.69601373\n",
      "Validation score: 0.685000\n",
      "Iteration 12, loss = 0.62783942\n",
      "Validation score: 0.698000\n",
      "Iteration 13, loss = 0.56067752\n",
      "Validation score: 0.723500\n",
      "Iteration 14, loss = 0.50965769\n",
      "Validation score: 0.724000\n",
      "Iteration 15, loss = 0.46895467\n",
      "Validation score: 0.722000\n",
      "Iteration 16, loss = 0.42861956\n",
      "Validation score: 0.740500\n",
      "Iteration 17, loss = 0.39184919\n",
      "Validation score: 0.742500\n",
      "Iteration 18, loss = 0.36282815\n",
      "Validation score: 0.763000\n",
      "Iteration 19, loss = 0.32768837\n",
      "Validation score: 0.762500\n",
      "Iteration 20, loss = 0.30096980\n",
      "Validation score: 0.778000\n",
      "Iteration 21, loss = 0.29192419\n",
      "Validation score: 0.775500\n",
      "Iteration 22, loss = 0.26591412\n",
      "Validation score: 0.771500\n",
      "Iteration 23, loss = 0.24428400\n",
      "Validation score: 0.782500\n",
      "Iteration 24, loss = 0.23177605\n",
      "Validation score: 0.780000\n",
      "Iteration 25, loss = 0.20780306\n",
      "Validation score: 0.794500\n",
      "Iteration 26, loss = 0.18706064\n",
      "Validation score: 0.792500\n",
      "Iteration 27, loss = 0.18597839\n",
      "Validation score: 0.782000\n",
      "Iteration 28, loss = 0.17977478\n",
      "Validation score: 0.802500\n",
      "Iteration 29, loss = 0.16069687\n",
      "Validation score: 0.804500\n",
      "Iteration 30, loss = 0.14441879\n",
      "Validation score: 0.810000\n",
      "Iteration 31, loss = 0.13878954\n",
      "Validation score: 0.813000\n",
      "Iteration 32, loss = 0.13272735\n",
      "Validation score: 0.800000\n",
      "Iteration 33, loss = 0.11836429\n",
      "Validation score: 0.816000\n",
      "Iteration 34, loss = 0.11476957\n",
      "Validation score: 0.809500\n",
      "Iteration 35, loss = 0.10749111\n",
      "Validation score: 0.817500\n",
      "Iteration 36, loss = 0.09950742\n",
      "Validation score: 0.812000\n",
      "Iteration 37, loss = 0.09780566\n",
      "Validation score: 0.813500\n",
      "Iteration 38, loss = 0.08641816\n",
      "Validation score: 0.822000\n",
      "Iteration 39, loss = 0.07998804\n",
      "Validation score: 0.817500\n",
      "Iteration 40, loss = 0.07600192\n",
      "Validation score: 0.816500\n",
      "Iteration 41, loss = 0.07052208\n",
      "Validation score: 0.825000\n",
      "Iteration 42, loss = 0.06771345\n",
      "Validation score: 0.817000\n",
      "Iteration 43, loss = 0.06202397\n",
      "Validation score: 0.817500\n",
      "Iteration 44, loss = 0.06009074\n",
      "Validation score: 0.817000\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "CPU times: user 20min 39s, sys: 6min 16s, total: 26min 56s\n",
      "Wall time: 6min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=512, learning_rate='adaptive',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nnc.fit(x, c.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5286949128524869 x train accuracy \n",
      " 0.41043083900226757 test seen accuracy \n",
      " 0.18233906302662622 test unseen accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nnc.score(x_train, c_train.reshape(-1)), 'x train accuracy \\n', \\\n",
    "      nnc.score(test_seen_x, test_seen_c.reshape(-1)), 'test seen accuracy \\n', \\\n",
    "      nnc.score(test_unseen_x, test_unseen_c.reshape(-1)), 'test unseen accuracy \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 x train accuracy \n",
      " 0.0 test seen accuracy \n",
      " 0.40613414223120997 test unseen accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nnc.score(x_train, c_train.reshape(-1)), 'x train accuracy \\n', \\\n",
    "      nnc.score(test_seen_x, test_seen_c.reshape(-1)), 'test seen accuracy \\n', \\\n",
    "      nnc.score(test_unseen_x, test_unseen_c.reshape(-1)), 'test unseen accuracy \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 x train accuracy \n",
      " 0.0 test seen accuracy \n",
      " 0.47960903269295585 test unseen accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nnc.score(x_train, c_train.reshape(-1)), 'x train accuracy \\n', \\\n",
    "      nnc.score(test_seen_x, test_seen_c.reshape(-1)), 'test seen accuracy \\n', \\\n",
    "      nnc.score(test_unseen_x, test_unseen_c.reshape(-1)), 'test unseen accuracy \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.401161966841434 x train accuracy \n",
      " 0.35090702947845803 test seen accuracy \n",
      " 0.20896528479946072 test unseen accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nnc.score(x_train, c_train.reshape(-1)), 'x train accuracy \\n', \\\n",
    "      nnc.score(test_seen_x, test_seen_c.reshape(-1)), 'test seen accuracy \\n', \\\n",
    "      nnc.score(test_unseen_x, test_unseen_c.reshape(-1)), 'test unseen accuracy \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(dual=False, C=1, max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52min 42s, sys: 14.3 s, total: 52min 56s\n",
      "Wall time: 52min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=20,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm.fit(x, c.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00555"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(x, c.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0042510982003684285"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(x_train, c_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002036659877800407"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(x_val, c_val.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002834467120181406"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(test_seen_x, test_seen_c.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = svm.score(test_unseen_x, test_unseen_c.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003370407819346141"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20805838344247857\n"
     ]
    }
   ],
   "source": [
    "n = len(un_cls)\n",
    "avg = 0\n",
    "for label in un_cls:\n",
    "    idx = np.where(test_unseen_c.reshape(-1) == label)[0]\n",
    "    avg += nnc.score(test_unseen_x[idx], test_unseen_c[idx])\n",
    "print(avg/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(svm, 'svc_vae_relu_l1_7_1_10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = joblib.load('svc_vae_relu_l1_4_2_10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchKernel",
   "language": "python",
   "name": "pyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
